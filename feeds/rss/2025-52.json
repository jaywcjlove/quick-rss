[
  {
    "id": "239",
    "url": "https://wangchujiang.com/quick-rss/issue/239.html",
    "title": "Osaurus：原生 Apple Silicon LLM 服务器，支持菜单栏、命令行和 macOS 26 的 Apple 模型。",
    "content_html": "<p><strong>Osaurus</strong> 是一款原生 macOS 的一体化 <strong>LLM 服务器</strong>，支持 <strong>MCP（Model Context Protocol）</strong>。\n它可以在 Apple Silicon 上同时运行 <strong>本地与远程大语言模型</strong>，并提供与 <strong>OpenAI 兼容的 API</strong>、工具调用能力以及内置插件生态。</p>\n<img alt=\"Osaurus\" src=\"https://github.com/user-attachments/assets/c16197bc-0b6d-461d-a77b-df4dd354e485\">\n<h2>什么是 Osaurus？</h2>\n<p>Osaurus 将多个关键能力整合到一个统一的 macOS 应用中，包括：</p>\n<ul>\n<li><strong>MLX 运行时</strong> —— 基于 MLX，为 Apple Silicon 优化的本地模型推理</li>\n<li><strong>远程模型提供商</strong> —— 可连接 OpenAI、OpenRouter、Ollama、LM Studio 以及任何兼容 OpenAI API 的服务</li>\n<li><strong>OpenAI / Anthropic / Ollama API 兼容</strong> —— 可直接作为现有工具的替代后端，无需改代码</li>\n<li><strong>MCP 服务器</strong> —— 通过 Model Context Protocol 向 AI Agent 暴露工具能力</li>\n<li><strong>远程 MCP 提供商</strong> —— 聚合外部 MCP 服务器的工具，统一管理</li>\n<li><strong>插件系统</strong> —— 支持社区插件和自定义工具扩展</li>\n<li><strong>开发者工具</strong> —— 内置请求分析、服务探索器，便于调试</li>\n<li><strong>Apple Foundation Models</strong> —— 在 macOS 26+（Tahoe）上直接使用系统级模型</li>\n</ul>\n<h2>功能一览</h2>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>功能</th><th>描述</th></tr></thead><tbody><tr><td>本地 LLM 服务器</td><td>本地运行 Llama、Qwen、Gemma、Mistral 等模型</td></tr><tr><td>远程模型支持</td><td>OpenAI、OpenRouter、Ollama、LM Studio 或自定义接口</td></tr><tr><td>OpenAI 兼容接口</td><td><code>/v1/chat/completions</code>，支持流式输出与工具调用</td></tr><tr><td>Anthropic 兼容</td><td><code>/messages</code> 接口，可直接用于 Claude Code 与官方 SDK</td></tr><tr><td>MCP 服务器</td><td>连接 Cursor、Claude Desktop 等 MCP 客户端</td></tr><tr><td>远程 MCP</td><td>聚合多个 MCP 服务器的工具能力</td></tr><tr><td>工具与插件</td><td>浏览器自动化、文件系统、Git、Web 搜索等</td></tr><tr><td>自定义主题</td><td>创建、导入、导出主题，支持完整配色定制</td></tr><tr><td>开发者工具</td><td>请求洞察、API Explorer、实时接口测试</td></tr><tr><td>菜单栏聊天</td><td>快捷聊天浮窗，支持会话历史与上下文追踪（⌘;）</td></tr><tr><td>模型管理</td><td>从 Hugging Face 下载并管理本地模型</td></tr></tbody></table>",
    "summary": "**Osaurus** 是一款原生 macOS 的一体化 **LLM 服务器**，支持 **MCP（Model Context Protocol）**。 它可以在 Apple Silicon 上同时运行 **本地与远程大语言模型**，并提供与 **OpenAI 兼容的 API**、工具调用能力以及内置插件生态。 ## 什么是 Osaurus？ Osaurus 将多个关键能力整合到一个统一的 mac",
    "banner_image": null,
    "date_published": "2025-12-20T06:01:04Z",
    "author": {
      "name": "jaywcjlove",
      "link": "https://avatars.githubusercontent.com/u/1680273?v=4"
    },
    "markdownContent": "**Osaurus** 是一款原生 macOS 的一体化 **LLM 服务器**，支持 **MCP（Model Context Protocol）**。\n它可以在 Apple Silicon 上同时运行 **本地与远程大语言模型**，并提供与 **OpenAI 兼容的 API**、工具调用能力以及内置插件生态。\n\n<img  alt=\"Osaurus\" src=\"https://github.com/user-attachments/assets/c16197bc-0b6d-461d-a77b-df4dd354e485\" />\n\n## 什么是 Osaurus？\n\nOsaurus 将多个关键能力整合到一个统一的 macOS 应用中，包括：\n\n* **MLX 运行时** —— 基于 MLX，为 Apple Silicon 优化的本地模型推理\n* **远程模型提供商** —— 可连接 OpenAI、OpenRouter、Ollama、LM Studio 以及任何兼容 OpenAI API 的服务\n* **OpenAI / Anthropic / Ollama API 兼容** —— 可直接作为现有工具的替代后端，无需改代码\n* **MCP 服务器** —— 通过 Model Context Protocol 向 AI Agent 暴露工具能力\n* **远程 MCP 提供商** —— 聚合外部 MCP 服务器的工具，统一管理\n* **插件系统** —— 支持社区插件和自定义工具扩展\n* **开发者工具** —— 内置请求分析、服务探索器，便于调试\n* **Apple Foundation Models** —— 在 macOS 26+（Tahoe）上直接使用系统级模型\n\n## 功能一览\n\n| 功能           | 描述                                        |\n| ------------ | ----------------------------------------- |\n| 本地 LLM 服务器   | 本地运行 Llama、Qwen、Gemma、Mistral 等模型         |\n| 远程模型支持       | OpenAI、OpenRouter、Ollama、LM Studio 或自定义接口 |\n| OpenAI 兼容接口  | `/v1/chat/completions`，支持流式输出与工具调用        |\n| Anthropic 兼容 | `/messages` 接口，可直接用于 Claude Code 与官方 SDK  |\n| MCP 服务器      | 连接 Cursor、Claude Desktop 等 MCP 客户端        |\n| 远程 MCP       | 聚合多个 MCP 服务器的工具能力                         |\n| 工具与插件        | 浏览器自动化、文件系统、Git、Web 搜索等                   |\n| 自定义主题        | 创建、导入、导出主题，支持完整配色定制                       |\n| 开发者工具        | 请求洞察、API Explorer、实时接口测试                  |\n| 菜单栏聊天        | 快捷聊天浮窗，支持会话历史与上下文追踪（⌘;）                   |\n| 模型管理         | 从 Hugging Face 下载并管理本地模型                  |"
  }
]

[
  {
    "id": "240",
    "url": "https://wangchujiang.com/quick-rss/issue/240.html",
    "title": "ProxyCast：把你的 AI 客户端额度用到任何地方",
    "content_html": "<h3>这个工具能帮你做什么？</h3>\n<p><strong>场景一：换个更好用的 IDE</strong>\n你可能已经有了 Kiro 账号，可以使用 Claude 系列模型，但如果 Kiro IDE 不太顺手，想尝试使用 <strong>Claude Code</strong> 或 <strong>Cursor</strong> 来写代码，而又不想额外付费购买 API，ProxyCast 可以帮你解决这个问题。</p>\n<p><strong>场景二：把额度分享给其他工具</strong>\n假设你有 <strong>Claude Code</strong> 这个月剩余的额度，为什么不利用它呢？你可以将剩余的额度转给其他工具，比如 <strong>Cherry Studio</strong> 用于聊天，或者为你的 <strong>AI Agent</strong> 项目提供 API 接口，避免浪费额度。</p>\n<p><strong>场景三：统一管理多个 AI 账号</strong>\n如果你有多个 AI 账户，比如 <strong>Kiro、Gemini CLI、通义千问</strong>，而且每个账户都有额度，想要统一管理这些账户，ProxyCast 可以根据不同的额度自动选择可用的账户，帮助你高效利用每个账号的额度。</p>\n<img alt=\"ProxyCast\" src=\"https://github.com/user-attachments/assets/e4e93c46-9fa6-4e6b-9bef-df6304660de9\">\n<img alt=\"ProxyCast\" src=\"https://github.com/user-attachments/assets/e7215ffd-ea80-4019-8568-89fcfde2cacb\">\n<img alt=\"ProxyCast\" src=\"https://github.com/user-attachments/assets/a4a7651c-a657-4b75-9705-0465763845ab\">\n<img alt=\"ProxyCast\" src=\"https://github.com/user-attachments/assets/60d82746-dd16-4311-83c8-3ae70fbc0050\">\n<img alt=\"ProxyCast\" src=\"https://github.com/user-attachments/assets/162bb8e2-f604-4a48-8537-6d962e922eb8\">\n<h3>✨ 核心特性</h3>\n<h4>🎯 多 Provider 统一管理</h4>\n<ul>\n<li><strong>Kiro</strong> - 通过 OAuth 使用 Claude 系列模型（Opus 4.5、Sonnet 4.5、Sonnet 4、Haiku 4.5）</li>\n<li><strong>Gemini CLI</strong> - 通过 OAuth 使用 Gemini 模型</li>\n<li><strong>Gemini API Key</strong> - 多账号负载均衡，支持模型排除</li>\n<li><strong>通义千问</strong> - 通过 OAuth 使用 Qwen3 Coder Plus</li>\n<li><strong>Antigravity</strong> - 通过 OAuth 使用 Claude 模型</li>\n<li><strong>Vertex AI</strong> - Google Cloud AI 平台，支持模型别名</li>\n<li><strong>OpenAI 自定义</strong> - 配置自定义 OpenAI 兼容 API</li>\n<li><strong>Claude 自定义</strong> - 配置自定义 Claude API</li>\n</ul>\n<h4>🖥️ 友好的图形界面</h4>\n<ul>\n<li><strong>Dashboard</strong> - 服务状态监控、API 测试面板</li>\n<li><strong>Provider 管理</strong> - 一键加载凭证、Token 刷新、默认 Provider 切换</li>\n<li><strong>设置页面</strong> - 服务器配置、端口设置、API Key 管理</li>\n<li><strong>日志查看</strong> - 实时日志记录、操作追踪</li>\n</ul>\n<h4>🔄 智能凭证管理</h4>\n<ul>\n<li>自动检测凭证文件变化（每 5 秒）</li>\n<li>一键读取本地 OAuth 凭证</li>\n<li>Token 过期自动刷新</li>\n<li>环境变量导出（.env 格式）</li>\n<li>配额超限自动切换 - 自动切换到下一个可用凭证</li>\n<li>预览模型回退 - 主模型配额用尽时尝试预览版本</li>\n<li><strong>Per-Key 代理</strong> - 为每个凭证单独配置代理</li>\n</ul>\n<h4>🔐 安全与管理</h4>\n<ul>\n<li><strong>TLS/HTTPS 支持</strong> - 可选启用 HTTPS 加密通信</li>\n<li><strong>远程管理 API</strong> - 通过 API 远程管理配置和凭证</li>\n<li><strong>访问控制</strong> - 支持 localhost 限制和密钥认证</li>\n</ul>\n<h4>🔌 多路由支持</h4>\n<ul>\n<li>支持 <code>/api/provider/{provider}/v1/*</code> 路由模式</li>\n<li><strong>模型映射</strong> - 将请求模型映射到 Provider 支持的模型</li>\n<li><strong>管理端点代理</strong> - 代理认证和账户功能</li>\n</ul>\n<h4>🌐 完整 API 兼容</h4>\n<ul>\n<li><strong>/v1/chat/completions</strong> - OpenAI Chat API</li>\n<li><strong>/v1/models</strong> - 模型列表</li>\n<li><strong>/v1/messages</strong> - Anthropic Messages API</li>\n<li><strong>/v1/messages/count_tokens</strong> - Token 计数</li>\n<li><strong>/api/provider/{provider}/v1/</strong> - Provider 路由</li>\n<li><strong>/v0/management/</strong> - 远程管理 API</li>\n</ul>",
    "summary": "### 这个工具能帮你做什么？ **场景一：换个更好用的 IDE** 你可能已经有了 Kiro 账号，可以使用 Claude 系列模型，但如果 Kiro IDE 不太顺手，想尝试使用 **Claude Code** 或 **Cursor** 来写代码，而又不想额外付费购买 API，ProxyCast 可以帮你解决这个问题。 **场景二：把额度分享给其他工具** 假设你有 **Claude Code",
    "banner_image": null,
    "date_published": "2025-12-20T19:06:34Z",
    "author": {
      "name": "jaywcjlove",
      "link": "https://avatars.githubusercontent.com/u/1680273?v=4"
    },
    "markdownContent": "### 这个工具能帮你做什么？\n\n**场景一：换个更好用的 IDE**\n你可能已经有了 Kiro 账号，可以使用 Claude 系列模型，但如果 Kiro IDE 不太顺手，想尝试使用 **Claude Code** 或 **Cursor** 来写代码，而又不想额外付费购买 API，ProxyCast 可以帮你解决这个问题。\n\n**场景二：把额度分享给其他工具**\n假设你有 **Claude Code** 这个月剩余的额度，为什么不利用它呢？你可以将剩余的额度转给其他工具，比如 **Cherry Studio** 用于聊天，或者为你的 **AI Agent** 项目提供 API 接口，避免浪费额度。\n\n**场景三：统一管理多个 AI 账号**\n如果你有多个 AI 账户，比如 **Kiro、Gemini CLI、通义千问**，而且每个账户都有额度，想要统一管理这些账户，ProxyCast 可以根据不同的额度自动选择可用的账户，帮助你高效利用每个账号的额度。\n\n<img alt=\"ProxyCast\" src=\"https://github.com/user-attachments/assets/e4e93c46-9fa6-4e6b-9bef-df6304660de9\" />\n\n<img alt=\"ProxyCast\" src=\"https://github.com/user-attachments/assets/e7215ffd-ea80-4019-8568-89fcfde2cacb\" />\n\n<img alt=\"ProxyCast\" src=\"https://github.com/user-attachments/assets/a4a7651c-a657-4b75-9705-0465763845ab\" />\n\n<img  alt=\"ProxyCast\" src=\"https://github.com/user-attachments/assets/60d82746-dd16-4311-83c8-3ae70fbc0050\" />\n\n<img  alt=\"ProxyCast\" src=\"https://github.com/user-attachments/assets/162bb8e2-f604-4a48-8537-6d962e922eb8\" />\n\n### ✨ 核心特性\n\n#### 🎯 多 Provider 统一管理\n\n* **Kiro** - 通过 OAuth 使用 Claude 系列模型（Opus 4.5、Sonnet 4.5、Sonnet 4、Haiku 4.5）\n* **Gemini CLI** - 通过 OAuth 使用 Gemini 模型\n* **Gemini API Key** - 多账号负载均衡，支持模型排除\n* **通义千问** - 通过 OAuth 使用 Qwen3 Coder Plus\n* **Antigravity** - 通过 OAuth 使用 Claude 模型\n* **Vertex AI** - Google Cloud AI 平台，支持模型别名\n* **OpenAI 自定义** - 配置自定义 OpenAI 兼容 API\n* **Claude 自定义** - 配置自定义 Claude API\n\n#### 🖥️ 友好的图形界面\n\n* **Dashboard** - 服务状态监控、API 测试面板\n* **Provider 管理** - 一键加载凭证、Token 刷新、默认 Provider 切换\n* **设置页面** - 服务器配置、端口设置、API Key 管理\n* **日志查看** - 实时日志记录、操作追踪\n\n#### 🔄 智能凭证管理\n\n* 自动检测凭证文件变化（每 5 秒）\n* 一键读取本地 OAuth 凭证\n* Token 过期自动刷新\n* 环境变量导出（.env 格式）\n* 配额超限自动切换 - 自动切换到下一个可用凭证\n* 预览模型回退 - 主模型配额用尽时尝试预览版本\n* **Per-Key 代理** - 为每个凭证单独配置代理\n\n#### 🔐 安全与管理\n\n* **TLS/HTTPS 支持** - 可选启用 HTTPS 加密通信\n* **远程管理 API** - 通过 API 远程管理配置和凭证\n* **访问控制** - 支持 localhost 限制和密钥认证\n\n#### 🔌 多路由支持\n\n* 支持 `/api/provider/{provider}/v1/*` 路由模式\n* **模型映射** - 将请求模型映射到 Provider 支持的模型\n* **管理端点代理** - 代理认证和账户功能\n\n#### 🌐 完整 API 兼容\n\n* **/v1/chat/completions** - OpenAI Chat API\n* **/v1/models** - 模型列表\n* **/v1/messages** - Anthropic Messages API\n* **/v1/messages/count_tokens** - Token 计数\n* **/api/provider/{provider}/v1/** - Provider 路由\n* **/v0/management/** - 远程管理 API"
  },
  {
    "id": "239",
    "url": "https://wangchujiang.com/quick-rss/issue/239.html",
    "title": "Osaurus：原生 Apple Silicon LLM 服务器，支持菜单栏、命令行和 macOS 26 的 Apple 模型。",
    "content_html": "<p><strong>Osaurus</strong> 是一款原生 macOS 的一体化 <strong>LLM 服务器</strong>，支持 <strong>MCP（Model Context Protocol）</strong>。\n它可以在 Apple Silicon 上同时运行 <strong>本地与远程大语言模型</strong>，并提供与 <strong>OpenAI 兼容的 API</strong>、工具调用能力以及内置插件生态。</p>\n<img alt=\"Osaurus\" src=\"https://github.com/user-attachments/assets/c16197bc-0b6d-461d-a77b-df4dd354e485\">\n<h2>什么是 Osaurus？</h2>\n<p>Osaurus 将多个关键能力整合到一个统一的 macOS 应用中，包括：</p>\n<ul>\n<li><strong>MLX 运行时</strong> —— 基于 MLX，为 Apple Silicon 优化的本地模型推理</li>\n<li><strong>远程模型提供商</strong> —— 可连接 OpenAI、OpenRouter、Ollama、LM Studio 以及任何兼容 OpenAI API 的服务</li>\n<li><strong>OpenAI / Anthropic / Ollama API 兼容</strong> —— 可直接作为现有工具的替代后端，无需改代码</li>\n<li><strong>MCP 服务器</strong> —— 通过 Model Context Protocol 向 AI Agent 暴露工具能力</li>\n<li><strong>远程 MCP 提供商</strong> —— 聚合外部 MCP 服务器的工具，统一管理</li>\n<li><strong>插件系统</strong> —— 支持社区插件和自定义工具扩展</li>\n<li><strong>开发者工具</strong> —— 内置请求分析、服务探索器，便于调试</li>\n<li><strong>Apple Foundation Models</strong> —— 在 macOS 26+（Tahoe）上直接使用系统级模型</li>\n</ul>\n<h2>功能一览</h2>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>功能</th><th>描述</th></tr></thead><tbody><tr><td>本地 LLM 服务器</td><td>本地运行 Llama、Qwen、Gemma、Mistral 等模型</td></tr><tr><td>远程模型支持</td><td>OpenAI、OpenRouter、Ollama、LM Studio 或自定义接口</td></tr><tr><td>OpenAI 兼容接口</td><td><code>/v1/chat/completions</code>，支持流式输出与工具调用</td></tr><tr><td>Anthropic 兼容</td><td><code>/messages</code> 接口，可直接用于 Claude Code 与官方 SDK</td></tr><tr><td>MCP 服务器</td><td>连接 Cursor、Claude Desktop 等 MCP 客户端</td></tr><tr><td>远程 MCP</td><td>聚合多个 MCP 服务器的工具能力</td></tr><tr><td>工具与插件</td><td>浏览器自动化、文件系统、Git、Web 搜索等</td></tr><tr><td>自定义主题</td><td>创建、导入、导出主题，支持完整配色定制</td></tr><tr><td>开发者工具</td><td>请求洞察、API Explorer、实时接口测试</td></tr><tr><td>菜单栏聊天</td><td>快捷聊天浮窗，支持会话历史与上下文追踪（⌘;）</td></tr><tr><td>模型管理</td><td>从 Hugging Face 下载并管理本地模型</td></tr></tbody></table>",
    "summary": "**Osaurus** 是一款原生 macOS 的一体化 **LLM 服务器**，支持 **MCP（Model Context Protocol）**。 它可以在 Apple Silicon 上同时运行 **本地与远程大语言模型**，并提供与 **OpenAI 兼容的 API**、工具调用能力以及内置插件生态。 ## 什么是 Osaurus？ Osaurus 将多个关键能力整合到一个统一的 mac",
    "banner_image": null,
    "date_published": "2025-12-20T06:01:04Z",
    "author": {
      "name": "jaywcjlove",
      "link": "https://avatars.githubusercontent.com/u/1680273?v=4"
    },
    "markdownContent": "**Osaurus** 是一款原生 macOS 的一体化 **LLM 服务器**，支持 **MCP（Model Context Protocol）**。\n它可以在 Apple Silicon 上同时运行 **本地与远程大语言模型**，并提供与 **OpenAI 兼容的 API**、工具调用能力以及内置插件生态。\n\n<img  alt=\"Osaurus\" src=\"https://github.com/user-attachments/assets/c16197bc-0b6d-461d-a77b-df4dd354e485\" />\n\n## 什么是 Osaurus？\n\nOsaurus 将多个关键能力整合到一个统一的 macOS 应用中，包括：\n\n* **MLX 运行时** —— 基于 MLX，为 Apple Silicon 优化的本地模型推理\n* **远程模型提供商** —— 可连接 OpenAI、OpenRouter、Ollama、LM Studio 以及任何兼容 OpenAI API 的服务\n* **OpenAI / Anthropic / Ollama API 兼容** —— 可直接作为现有工具的替代后端，无需改代码\n* **MCP 服务器** —— 通过 Model Context Protocol 向 AI Agent 暴露工具能力\n* **远程 MCP 提供商** —— 聚合外部 MCP 服务器的工具，统一管理\n* **插件系统** —— 支持社区插件和自定义工具扩展\n* **开发者工具** —— 内置请求分析、服务探索器，便于调试\n* **Apple Foundation Models** —— 在 macOS 26+（Tahoe）上直接使用系统级模型\n\n## 功能一览\n\n| 功能           | 描述                                        |\n| ------------ | ----------------------------------------- |\n| 本地 LLM 服务器   | 本地运行 Llama、Qwen、Gemma、Mistral 等模型         |\n| 远程模型支持       | OpenAI、OpenRouter、Ollama、LM Studio 或自定义接口 |\n| OpenAI 兼容接口  | `/v1/chat/completions`，支持流式输出与工具调用        |\n| Anthropic 兼容 | `/messages` 接口，可直接用于 Claude Code 与官方 SDK  |\n| MCP 服务器      | 连接 Cursor、Claude Desktop 等 MCP 客户端        |\n| 远程 MCP       | 聚合多个 MCP 服务器的工具能力                         |\n| 工具与插件        | 浏览器自动化、文件系统、Git、Web 搜索等                   |\n| 自定义主题        | 创建、导入、导出主题，支持完整配色定制                       |\n| 开发者工具        | 请求洞察、API Explorer、实时接口测试                  |\n| 菜单栏聊天        | 快捷聊天浮窗，支持会话历史与上下文追踪（⌘;）                   |\n| 模型管理         | 从 Hugging Face 下载并管理本地模型                  |"
  }
]
